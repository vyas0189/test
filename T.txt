import chainlit as cl
import httpx
import asyncio
from typing import AsyncGenerator, List, Dict, Any
from datetime import datetime
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# LLM API endpoint
LLM_API_URL = "http://localhost:8080/llm"
MAX_HISTORY_ITEMS = 50

# Global variables for status and conversation management
api_status = {"status": "unknown", "last_check": None, "response_time": None}

class ConversationManager:
    def __init__(self):
        self.conversations: Dict[str, List[Dict]] = {}
    
    def add_message(self, session_id: str, role: str, content: str):
        if session_id not in self.conversations:
            self.conversations[session_id] = []
        
        message = {
            "role": role,
            "content": content,
            "timestamp": datetime.now().isoformat()
        }
        
        self.conversations[session_id].append(message)
        
        # Keep only the last MAX_HISTORY_ITEMS messages
        if len(self.conversations[session_id]) > MAX_HISTORY_ITEMS:
            self.conversations[session_id] = self.conversations[session_id][-MAX_HISTORY_ITEMS:]
    
    def get_conversation(self, session_id: str) -> List[Dict]:
        return self.conversations.get(session_id, [])
    
    def format_conversation_for_api(self, session_id: str) -> str:
        """Format conversation history as a string for the API"""
        conversation = self.get_conversation(session_id)
        if not conversation:
            return ""
        
        formatted = []
        for msg in conversation[-10:]:  # Last 10 messages for context
            role = "User" if msg['role'] == 'user' else "Assistant"
            formatted.append(f"{role}: {msg['content']}")
        
        return "\n".join(formatted)

# Initialize conversation manager
conv_manager = ConversationManager()

async def show_thinking_animation():
    """Create a thinking animation with dots"""
    thinking_states = [
        "ü§î Thinking",
        "ü§î Thinking.",
        "ü§î Thinking..",
        "ü§î Thinking..."
    ]
    for state in thinking_states:
        yield state
        await asyncio.sleep(0.5)

async def check_api_status():
    """Check if the LLM API is available"""
    global api_status
    try:
        start_time = datetime.now()
        async with httpx.AsyncClient() as client:
            # Try to make a simple request to check connectivity
            response = await client.get(f"{LLM_API_URL.replace('/llm', '')}/health", timeout=5.0)
            response_time = (datetime.now() - start_time).total_seconds()
            
            if response.status_code == 200:
                api_status = {
                    "status": "online",
                    "last_check": datetime.now().isoformat(),
                    "response_time": response_time
                }
            else:
                api_status = {
                    "status": "error",
                    "last_check": datetime.now().isoformat(),
                    "error": f"HTTP {response.status_code}"
                }
    except httpx.RequestError as e:
        api_status = {
            "status": "offline",
            "last_check": datetime.now().isoformat(),
            "error": str(e)
        }
    except Exception as e:
        api_status = {
            "status": "error",
            "last_check": datetime.now().isoformat(),
            "error": str(e)
        }

async def stream_from_llm_api(prompt: str, conversation_context: str = "") -> AsyncGenerator[str, None]:
    """Stream response from the LLM API"""
    try:
        # Update API status before making request
        await check_api_status()
        
        # Prepare the full prompt with conversation context
        full_prompt = f"{conversation_context}\nUser: {prompt}\nAssistant:" if conversation_context else prompt
        
        async with httpx.AsyncClient() as client:
            async with client.stream(
                "POST", 
                LLM_API_URL,
                json={"prompt": full_prompt},
                headers={"Content-Type": "application/json"},
                timeout=60.0
            ) as response:
                # Check if the request was successful
                response.raise_for_status()
                
                # Update status to online since request succeeded
                api_status["status"] = "online"
                api_status["last_check"] = datetime.now().isoformat()
                
                # Process the streaming response direct content without JSON wrapping
                async for chunk in response.aiter_text():
                    # The API now returns raw markdown content directly
                    if chunk:
                        yield chunk
                        
    except httpx.HTTPError as e:
        logger.error(f"HTTP error from LLM API: {e}")
        api_status["status"] = "error"
        api_status["last_check"] = datetime.now().isoformat()
        api_status["error"] = str(e)
        yield f"‚ùå Error connecting to LLM API: {str(e)}"
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        api_status["status"] = "error"
        api_status["last_check"] = datetime.now().isoformat()
        api_status["error"] = str(e)
        yield f"‚ùå Unexpected error: {str(e)}"

def create_sidebar_html(session_id: str) -> str:
    """Create HTML for the sidebar with status and history"""
    
    # Status section
    status_icons = {
        "online": "üü¢",
        "offline": "üî¥", 
        "error": "üü°",
        "unknown": "‚ö´"
    }
    
    status_icon = status_icons.get(api_status["status"], "‚ö´")
    status_text = api_status["status"].upper()
    
    status_html = f"""
    <div class="status-section">
        <h3>üìä API Status</h3>
        <div class="status-item">
            <span class="status-indicator">{status_icon}</span>
            <strong>{status_text}</strong>
        </div>
        <div class="status-detail">‚è∞ Last Check: {api_status.get('last_check', 'Never')}</div>
    """
    
    if api_status.get('response_time'):
        status_html += f'<div class="status-detail">‚ö° Response Time: {api_status["response_time"]:.3f}s</div>'
    
    if api_status.get('error'):
        status_html += f'<div class="status-error">‚ùå Error: {api_status["error"]}</div>'
    
    status_html += "</div>"
    
    # History section
    conversation = conv_manager.get_conversation(session_id)
    
    history_html = """
    <div class="history-section">
        <h3>üí¨ Conversation History</h3>
        <div class="history-container">
    """
    
    if not conversation:
        history_html += '<div class="no-history">No messages yet. Start chatting!</div>'
    else:
        # Show last 8 messages in history panel
        recent_messages = conversation[-8:]
        
        for msg in recent_messages:
            timestamp = datetime.fromisoformat(msg['timestamp']).strftime("%H:%M")
            role_icon = "üë§" if msg['role'] == 'user' else "ü§ñ"
            role_class = "user" if msg['role'] == 'user' else "assistant"
            
            # Truncate long messages for display
            content_preview = msg['content']
            if len(content_preview) > 60:
                content_preview = content_preview[:60] + "..."
            
            history_html += f"""
            <div class="history-item {role_class}">
                <div class="history-header">
                    <span class="role-icon">{role_icon}</span>
                    <span class="timestamp">{timestamp}</span>
                </div>
                <div class="history-content">{content_preview}</div>
            </div>
            """
    
    history_html += """
        </div>
    </div>
    """
    
    # Combine everything
    sidebar_html = f"""
    <div class="sidebar">
        {status_html}
        {history_html}
    </div>
    """
    
    return sidebar_html

def create_main_layout_css() -> str:
    """Create CSS for the main layout"""
    return """
    <style>
    .sidebar {
        position: fixed;
        left: 0;
        top: 0;
        width: 300px;
        height: 100vh;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 20px;
        box-sizing: border-box;
        overflow-y: auto;
        z-index: 1000;
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
    }
    
    .status-section {
        background: rgba(255, 255, 255, 0.1);
        border-radius: 10px;
        padding: 15px;
        margin-bottom: 20px;
        backdrop-filter: blur(10px);
    }
    
    .status-section h3 {
        margin: 0 0 15px 0;
        font-size: 16px;
        font-weight: 600;
    }
    
    .status-item {
        display: flex;
        align-items: center;
        gap: 8px;
        margin-bottom: 8px;
        font-size: 14px;
    }
    
    .status-indicator {
        font-size: 16px;
    }
    
    .status-detail {
        font-size: 12px;
        opacity: 0.8;
        margin-bottom: 4px;
    }
    
    .status-error {
        font-size: 12px;
        color: #ffcccb;
        margin-top: 8px;
    }
    
    .history-section {
        background: rgba(255, 255, 255, 0.1);
        border-radius: 10px;
        padding: 15px;
        backdrop-filter: blur(10px);
    }
    
    .history-section h3 {
        margin: 0 0 15px 0;
        font-size: 16px;
        font-weight: 600;
    }
    
    .history-container {
        max-height: 400px;
        overflow-y: auto;
    }
    
    .history-item {
        background: rgba(255, 255, 255, 0.1);
        border-radius: 8px;
        padding: 10px;
        margin-bottom: 8px;
        border-left: 3px solid transparent;
    }
    
    .history-item.user {
        border-left-color: #4CAF50;
    }
    
    .history-item.assistant {
        border-left-color: #2196F3;
    }
    
    .history-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        margin-bottom: 5px;
    }
    
    .role-icon {
        font-size: 14px;
    }
    
    .timestamp {
        font-size: 11px;
        opacity: 0.7;
    }
    
    .history-content {
        font-size: 12px;
        line-height: 1.4;
        opacity: 0.9;
    }
    
    .no-history {
        text-align: center;
        font-style: italic;
        opacity: 0.7;
        padding: 20px;
    }
    
    /* Thinking animation */
    .thinking-message {
        font-style: italic;
        color: #888;
        animation: pulse 1.5s ease-in-out infinite;
    }
    
    @keyframes pulse {
        0% { opacity: 0.6; }
        50% { opacity: 1; }
        100% { opacity: 0.6; }
    }
    
    /* Adjust main content to account for sidebar */
    .main-container {
        margin-left: 320px;
        padding: 20px;
    }
    
    /* Override ChainLit's default styles */
    .chainlit-prose {
        margin-left: 320px !important;
    }
    
    /* Responsive design */
    @media (max-width: 768px) {
        .sidebar {
            width: 100%;
            height: auto;
            position: relative;
        }
        
        .main-container,
        .chainlit-prose {
            margin-left: 0 !important;
        }
    }
    </style>
    """

async def send_sidebar_update(session_id: str):
    """Send updated sidebar"""
    await check_api_status()
    
    sidebar_html = create_sidebar_html(session_id)
    css = create_main_layout_css()
    
    # Create HTML element with the sidebar
    sidebar_element = cl.Html(content=css + sidebar_html, name="sidebar")
    
    # Send as a message with the sidebar
    await cl.Message(
        content="",
        elements=[sidebar_element]
    ).send()

async def periodic_status_check():
    """Periodically check API status"""
    while True:
        await asyncio.sleep(30)  # Check every 30 seconds
        old_status = api_status.get("status")
        await check_api_status()
        
        # Log status changes
        if api_status.get("status") != old_status:
            logger.info(f"API status changed from {old_status} to {api_status.get('status')}")

@cl.on_chat_start
async def start():
    """This function is called when a new chat starts."""
    session_id = cl.user_session.get("id", "default")
    
    # Initial status check
    await check_api_status()
    
    # Create and send initial sidebar
    await send_sidebar_update(session_id)
    
    # Send welcome message
    await cl.Message(
        content="Hello! I'm your AI assistant powered by a local LLM API. How can I help you today? ü§ñ\n\n*Check the sidebar on the left for API status and conversation history.*"
    ).send()
    
    # Start periodic status checks
    asyncio.create_task(periodic_status_check())

# VERSION 1: Using Steps (More Professional)
@cl.on_message
async def main_with_steps(message: cl.Message):
    """Version with thinking steps - rename this to 'main' to use"""
    session_id = cl.user_session.get("id", "default")
    user_message = message.content
    
    # Add user message to conversation history
    conv_manager.add_message(session_id, "user", user_message)
    
    # Get conversation context for the API
    conversation_context = conv_manager.format_conversation_for_api(session_id)
    
    # Show thinking indicator with step
    async with cl.Step(name="ü§î Thinking...") as step:
        step.output = "üîç Processing your message and preparing response..."
        
        # Initialize a new message with empty content for streaming
        msg = cl.Message(content="")
        full_response = ""
        
        try:
            # Send the prompt and stream the response
            response_started = False
            async for text_chunk in stream_from_llm_api(user_message, conversation_context):
                if not response_started:
                    # Update step when we start receiving response
                    step.output = "‚ú® Generating response..."
                    response_started = True
                
                # Update the message content as chunks arrive
                full_response += text_chunk
                await msg.stream_token(text_chunk)
            
            # Update step when complete
            if full_response:
                step.output = "‚úÖ Response generated successfully!"
            else:
                step.output = "‚ö†Ô∏è No response received from API"
            
            # Send the final message
            await msg.send()
            
            # Add AI response to conversation history
            conv_manager.add_message(session_id, "assistant", full_response)
            
            # Update sidebar with new conversation
            await send_sidebar_update(session_id)
            
        except Exception as e:
            # Update step with error and handle the exception
            step.output = f"‚ùå Error: {str(e)}"
            error_message = f"‚ùå Error processing your message: {str(e)}"
            await cl.Message(content=error_message).send()
            logger.error(f"Error in main handler: {e}")

# VERSION 2: Using Typing Message (More Dynamic)
@cl.on_message
async def main(message: cl.Message):
    """Version with typing indicator message"""
    session_id = cl.user_session.get("id", "default")
    user_message = message.content
    
    # Add user message to conversation history
    conv_manager.add_message(session_id, "user", user_message)
    
    # Get conversation context for the API
    conversation_context = conv_manager.format_conversation_for_api(session_id)
    
    # Create thinking message
    thinking_msg = cl.Message(content="ü§î Thinking...")
    await thinking_msg.send()
    
    # Start thinking animation task
    thinking_task = asyncio.create_task(animate_thinking(thinking_msg))
    
    try:
        # Initialize a new message with empty content for streaming
        msg = cl.Message(content="")
        full_response = ""
        
        # Send the prompt and stream the response
        response_started = False
        async for text_chunk in stream_from_llm_api(user_message, conversation_context):
            if not response_started:
                # Cancel thinking animation and update message
                thinking_task.cancel()
                await thinking_msg.remove()
                response_started = True
            
            # Update the message content as chunks arrive
            full_response += text_chunk
            await msg.stream_token(text_chunk)
        
        # Send the final message
        await msg.send()
        
        # Add AI response to conversation history
        conv_manager.add_message(session_id, "assistant", full_response)
        
        # Update sidebar with new conversation
        await send_sidebar_update(session_id)
        
    except Exception as e:
        # Cancel thinking animation and show error
        thinking_task.cancel()
        await thinking_msg.remove()
        error_message = f"‚ùå Error processing your message: {str(e)}"
        await cl.Message(content=error_message).send()
        logger.error(f"Error in main handler: {e}")

async def animate_thinking(thinking_msg: cl.Message):
    """Animate the thinking message with dots"""
    try:
        thinking_states = [
            "ü§î Thinking",
            "ü§î Thinking.",
            "ü§î Thinking..",
            "ü§î Thinking...",
            "üß† Processing",
            "üß† Processing.",
            "üß† Processing..",
            "üß† Processing...",
            "‚ú® Generating",
            "‚ú® Generating.",
            "‚ú® Generating..",
            "‚ú® Generating..."
        ]
        
        while True:
            for state in thinking_states:
                await thinking_msg.update(content=state)
                await asyncio.sleep(0.8)
                
    except asyncio.CancelledError:
        # Animation was cancelled (response started)
        pass

# Optional: Add settings for API configuration
@cl.on_settings_update
async def setup_agent(settings):
    """Handle settings updates"""
    logger.info(f"Settings updated: {settings}")

if __name__ == "__main__":
    # This would typically be run with: chainlit run app.py
    logger.info("Starting ChainLit application...")
    logger.info(f"LLM API URL: {LLM_API_URL}")
