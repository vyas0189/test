import chainlit as cl
import httpx
import asyncio
from typing import AsyncGenerator, List, Dict, Any
from datetime import datetime
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# LLM API endpoint (fixed the typo from "11m" to "llm")
LLM_API_URL = "http://localhost:8080/llm"
MAX_HISTORY_ITEMS = 50

# Global variables for status and conversation management
api_status = {"status": "unknown", "last_check": None, "response_time": None}

class ConversationManager:
    def __init__(self):
        self.conversations: Dict[str, List[Dict]] = {}
    
    def add_message(self, session_id: str, role: str, content: str):
        if session_id not in self.conversations:
            self.conversations[session_id] = []
        
        message = {
            "role": role,
            "content": content,
            "timestamp": datetime.now().isoformat()
        }
        
        self.conversations[session_id].append(message)
        
        # Keep only the last MAX_HISTORY_ITEMS messages
        if len(self.conversations[session_id]) > MAX_HISTORY_ITEMS:
            self.conversations[session_id] = self.conversations[session_id][-MAX_HISTORY_ITEMS:]
    
    def get_conversation(self, session_id: str) -> List[Dict]:
        return self.conversations.get(session_id, [])
    
    def format_conversation_for_api(self, session_id: str) -> str:
        """Format conversation history as a string for the API"""
        conversation = self.get_conversation(session_id)
        if not conversation:
            return ""
        
        formatted = []
        for msg in conversation[-10:]:  # Last 10 messages for context
            role = "User" if msg['role'] == 'user' else "Assistant"
            formatted.append(f"{role}: {msg['content']}")
        
        return "\n".join(formatted)

# Initialize conversation manager
conv_manager = ConversationManager()

async def check_api_status():
    """Check if the LLM API is available"""
    global api_status
    try:
        start_time = datetime.now()
        async with httpx.AsyncClient() as client:
            # Try to make a simple request to check connectivity
            response = await client.get(f"{LLM_API_URL.replace('/llm', '')}/health", timeout=5.0)
            response_time = (datetime.now() - start_time).total_seconds()
            
            if response.status_code == 200:
                api_status = {
                    "status": "online",
                    "last_check": datetime.now().isoformat(),
                    "response_time": response_time
                }
            else:
                api_status = {
                    "status": "error",
                    "last_check": datetime.now().isoformat(),
                    "error": f"HTTP {response.status_code}"
                }
    except httpx.RequestError as e:
        api_status = {
            "status": "offline",
            "last_check": datetime.now().isoformat(),
            "error": str(e)
        }
    except Exception as e:
        api_status = {
            "status": "error",
            "last_check": datetime.now().isoformat(),
            "error": str(e)
        }

async def stream_from_llm_api(prompt: str, conversation_context: str = "") -> AsyncGenerator[str, None]:
    """Stream response from the LLM API"""
    try:
        # Update API status before making request
        await check_api_status()
        
        # Prepare the full prompt with conversation context
        full_prompt = f"{conversation_context}\nUser: {prompt}\nAssistant:" if conversation_context else prompt
        
        async with httpx.AsyncClient() as client:
            async with client.stream(
                "POST", 
                LLM_API_URL,
                json={"prompt": full_prompt},
                headers={"Content-Type": "application/json"},
                timeout=60.0
            ) as response:
                # Check if the request was successful
                response.raise_for_status()
                
                # Update status to online since request succeeded
                api_status["status"] = "online"
                api_status["last_check"] = datetime.now().isoformat()
                
                # Process the streaming response direct content without JSON wrapping
                async for chunk in response.aiter_text():
                    # The API now returns raw markdown content directly
                    if chunk:
                        yield chunk
                        
    except httpx.HTTPError as e:
        logger.error(f"HTTP error from LLM API: {e}")
        api_status["status"] = "error"
        api_status["last_check"] = datetime.now().isoformat()
        api_status["error"] = str(e)
        yield f"‚ùå Error connecting to LLM API: {str(e)}"
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        api_status["status"] = "error"
        api_status["last_check"] = datetime.now().isoformat()
        api_status["error"] = str(e)
        yield f"‚ùå Unexpected error: {str(e)}"

def create_status_element():
    """Create a status display element"""
    status_icons = {
        "online": "üü¢",
        "offline": "üî¥", 
        "error": "üü°",
        "unknown": "‚ö´"
    }
    
    status_icon = status_icons.get(api_status["status"], "‚ö´")
    status_text = api_status["status"].upper()
    
    content = f"""## üìä API Status
    
{status_icon} **Status:** {status_text}

‚è∞ **Last Check:** {api_status.get('last_check', 'Never')}"""

    if api_status.get('response_time'):
        content += f"\n‚ö° **Response Time:** {api_status['response_time']:.3f}s"
    
    if api_status.get('error'):
        content += f"\n‚ùå **Error:** {api_status['error']}"
    
    return cl.Text(content=content, name="api_status", display="side")

def create_history_element(session_id: str):
    """Create conversation history element"""
    conversation = conv_manager.get_conversation(session_id)
    
    if not conversation:
        content = """## üí¨ Conversation History
        
*No messages yet. Start chatting!*"""
    else:
        content = "## üí¨ Conversation History\n\n"
        
        # Show last 8 messages in history panel
        recent_messages = conversation[-8:]
        
        for msg in recent_messages:
            timestamp = datetime.fromisoformat(msg['timestamp']).strftime("%H:%M")
            role_icon = "üë§" if msg['role'] == 'user' else "ü§ñ"
            
            # Truncate long messages for display
            content_preview = msg['content']
            if len(content_preview) > 60:
                content_preview = content_preview[:60] + "..."
            
            content += f"**{timestamp}** {role_icon} {content_preview}\n\n"
    
    return cl.Text(content=content, name="conversation_history", display="side")

async def update_sidebar_elements(session_id: str):
    """Update both status and history elements"""
    await check_api_status()
    
    status_element = create_status_element()
    history_element = create_history_element(session_id)
    
    # Update elements in the UI
    await status_element.send()
    await history_element.send()

@cl.on_chat_start
async def start():
    """This function is called when a new chat starts."""
    session_id = cl.user_session.get("id", "default")
    
    # Initial status check
    await check_api_status()
    
    # Create and send initial sidebar elements
    status_element = create_status_element()
    history_element = create_history_element(session_id)
    
    await status_element.send()
    await history_element.send()
    
    # Send welcome message
    await cl.Message(
        content="Hello! I'm your AI assistant powered by a local LLM API. How can I help you today? ü§ñ\n\n*Check the sidebar for API status and conversation history.*"
    ).send()
    
    # Start periodic status checks
    asyncio.create_task(periodic_status_check())

async def periodic_status_check():
    """Periodically check API status and update UI"""
    while True:
        await asyncio.sleep(30)  # Check every 30 seconds
        session_id = cl.user_session.get("id", "default")
        
        old_status = api_status.get("status")
        await check_api_status()
        
        # Only update UI if status changed
        if api_status.get("status") != old_status:
            try:
                status_element = create_status_element()
                await status_element.send()
            except Exception as e:
                logger.error(f"Error updating status UI: {e}")

@cl.on_message
async def main(message: cl.Message):
    """This function is called every time a user sends a message."""
    session_id = cl.user_session.get("id", "default")
    user_message = message.content
    
    # Add user message to conversation history
    conv_manager.add_message(session_id, "user", user_message)
    
    # Get conversation context for the API
    conversation_context = conv_manager.format_conversation_for_api(session_id)
    
    # Initialize a new message with empty content for streaming
    msg = cl.Message(content="")
    
    full_response = ""
    
    try:
        # Send the prompt and stream the response
        async for text_chunk in stream_from_llm_api(user_message, conversation_context):
            # Update the message content as chunks arrive
            full_response += text_chunk
            await msg.stream_token(text_chunk)
        
        # Send the final message
        await msg.send()
        
        # Add AI response to conversation history
        conv_manager.add_message(session_id, "assistant", full_response)
        
        # Update sidebar elements
        await update_sidebar_elements(session_id)
        
    except Exception as e:
        # Handle any unexpected errors
        error_message = f"‚ùå Error processing your message: {str(e)}"
        await cl.Message(content=error_message).send()
        logger.error(f"Error in main handler: {e}")

# Optional: Add settings for API configuration
@cl.on_settings_update
async def setup_agent(settings):
    """Handle settings updates"""
    logger.info(f"Settings updated: {settings}")

if __name__ == "__main__":
    # This would typically be run with: chainlit run app.py
    logger.info("Starting ChainLit application...")
    logger.info(f"LLM API URL: {LLM_API_URL}")
